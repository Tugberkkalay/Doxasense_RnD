# ğŸš€ Runpod GPU Worker - Deployment KÄ±lavuzu

DoxaSense-MIND iÃ§in Runpod Serverless GPU worker deployment rehberi.

---

## ğŸ“ DOSYALAR

```
/app/runpod/
â”œâ”€â”€ Dockerfile              # Docker image tanÄ±mÄ±
â”œâ”€â”€ handler.py              # GPU worker handler
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ build_and_push.sh       # Build & deploy script
â””â”€â”€ README.md              # Bu dosya
```

---

## ğŸ¯ HIZLI BAÅLANGIÃ‡ (3 YÃ¶ntem)

### **YÃ–NTEM 1: HazÄ±r Template (EN HIZLI - 5 dakika)**

Åimdilik kendi Docker image'ini build etmeden Runpod'un hazÄ±r template'ini kullan:

1. **Runpod Console'a git:**
   ```
   https://www.runpod.io/console/serverless
   ```

2. **"New Endpoint" tÄ±kla**

3. **"Import from Docker Registry" seÃ§:**
   ```
   Docker Image: runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel
   ```

4. **GPU SeÃ§:**
   ```
   GPU: RTX A4000 (16GB) - Spot $0.25/hr
   ```

5. **Configuration:**
   ```
   Name: doxasense-worker
   Min Workers: 0
   Max Workers: 2
   GPUs per Worker: 1
   Idle Timeout: 30 seconds
   ```

6. **Deploy! â†’ Endpoint URL'i kopyala**

**Endpoint URL Ã–rneÄŸi:**
```
https://api.runpod.ai/v2/abc123xyz456
```

**Bu URL'i DoxaSense'e ekle:**
```bash
# .env dosyasÄ±na:
RUNPOD_ENDPOINT=https://api.runpod.ai/v2/abc123xyz456
```

**Not:** HazÄ±r template ile modeller ilk Ã§alÄ±ÅŸmada inecek (yavaÅŸ), ama test iÃ§in yeterli.

---

### **YÃ–NTEM 2: Custom Docker Image (Ã–NERÄ°LEN - 45 dakika)**

Kendi Docker image'ini build et (modeller pre-loaded):

#### **A. Local'de Build & Push**

**Gereksinimler:**
- Docker Desktop kurulu
- Docker Hub hesabÄ±
- 20 GB boÅŸ disk alanÄ±

**Komutlar:**

```bash
# 1. Bu dizine git
cd /app/runpod

# 2. Build script'i Ã§alÄ±ÅŸtÄ±rÄ±labilir yap
chmod +x build_and_push.sh

# 3. Build ve push (Docker Hub username'inle)
./build_and_push.sh tugberkkalay

# Script ne yapar:
# - Docker image build eder (~25 dakika)
# - Modelleri image'e gÃ¶mÃ¼lÃ¼r (~15 GB)
# - Docker Hub'a push eder (~15 dakika)
```

**Manuel olarak:**
```bash
# Build
docker build -t tugberkkalay/doxasense-gpu-worker:latest .

# Login
docker login

# Push
docker push tugberkkalay/doxasense-gpu-worker:latest
```

#### **B. Runpod'da Deploy**

1. **Runpod Console â†’ Serverless â†’ New Endpoint**

2. **"Import from Docker Registry" seÃ§**

3. **Docker Image:**
   ```
   tugberkkalay/doxasense-gpu-worker:latest
   ```

4. **GPU & Config** (aynÄ± yukarÄ±daki gibi)

5. **Deploy!**

---

### **YÃ–NTEM 3: Runpod GitHub Integration (EN KOLAY)**

Runpod direkt GitHub'dan build edebilir:

#### **A. GitHub'a Push**

```bash
# Local'de:
cd /app
git init
git add runpod/
git commit -m "Runpod GPU worker"
git remote add origin https://github.com/tugberkkalay/doxasense-runpod.git
git push -u origin main
```

#### **B. Runpod'da Import**

1. **Runpod Console â†’ Serverless â†’ New Endpoint**

2. **"Import GitHub Repository" seÃ§**

3. **Repository:**
   ```
   tugberkkalay/doxasense-runpod
   ```

4. **Dockerfile Path:**
   ```
   runpod/Dockerfile
   ```

5. **Deploy!**

Runpod otomatik build edecek (25-30 dakika).

---

## ğŸ“¦ DOSYALARI LOCAL'E Ä°NDÄ°RME

Local makinende build etmek iÃ§in:

### **YÃ¶ntem A: Git Clone**
```bash
# EÄŸer GitHub'a pushlamissan:
git clone https://github.com/tugberkkalay/doxasense.git
cd doxasense/runpod
./build_and_push.sh tugberkkalay
```

### **YÃ¶ntem B: Manuel Kopyala**

1. **DosyalarÄ± local'e indir:**
   - `/app/runpod/Dockerfile`
   - `/app/runpod/handler.py`
   - `/app/runpod/requirements.txt`

2. **Bir klasÃ¶re koy:**
   ```
   ~/doxasense-runpod/
   â”œâ”€â”€ Dockerfile
   â”œâ”€â”€ handler.py
   â””â”€â”€ requirements.txt
   ```

3. **Build:**
   ```bash
   cd ~/doxasense-runpod
   docker build -t tugberkkalay/doxasense-gpu:latest .
   ```

4. **Push:**
   ```bash
   docker login
   docker push tugberkkalay/doxasense-gpu:latest
   ```

---

## âš¡ RUNPOD'DA DEPLOY (Resimli AdÄ±mlar)

### 1. Serverless Sekmesi
```
Sol menÃ¼ â†’ Serverless (âš¡ ikonu)
```

### 2. New Endpoint
```
SaÄŸ Ã¼st â†’ "+ New Endpoint"
```

### 3. Import Docker Image
```
"Import from Docker Registry" kartÄ±na tÄ±kla

Docker Image Name:
  tugberkkalay/doxasense-gpu-worker:latest
  
Container Registry Credentials: (boÅŸ bÄ±rak - public image)
```

### 4. Select GPU
```
GPU Type: RTX A4000
- 16 GB VRAM
- Spot: $0.25/hr
- On-Demand: $0.34/hr

Ã–nerilen: Spot (daha ucuz, yeterli availability)
```

### 5. Configure Endpoint
```
Endpoint Name: doxasense-gpu

Active Workers:
  Min: 0  (idle'da worker yok, maliyet $0)
  Max: 3  (aynÄ± anda 3 paralel iÅŸlem)

GPUs Per Worker: 1

Advanced Config:
  Idle Timeout: 30 seconds (iÅŸlem bitmeden 30s sonra shutdown)
  Execution Timeout: 300 seconds (max 5 dakika/iÅŸlem)
  
Flashboot: âœ“ Enable (daha hÄ±zlÄ± cold start)
```

### 6. Deploy
```
"Deploy Endpoint" butonuna tÄ±kla
```

Deployment 2-3 dakika sÃ¼rer.

### 7. Endpoint URL Al
```
Deploy sonrasÄ±:

Endpoint Details sayfasÄ±nda:
  Endpoint URL: https://api.runpod.ai/v2/abc123xyz456
  
Bu URL'i kopyala!
```

---

## ğŸ”— UYGULAMAYA ENTEGRASYON

Endpoint URL'ini aldÄ±ktan sonra:

### 1. Environment Variable Ekle
```bash
# /app/.env dosyasÄ±na:
RUNPOD_ENDPOINT=https://api.runpod.ai/v2/abc123xyz456
```

### 2. Backend Restart
```bash
supervisorctl restart backend
```

### 3. Test!
```bash
# GPU ile iÅŸle:
curl -X POST http://localhost:8001/api/ingest/upload?use_gpu=true \
  -F "file=@test.pdf"

# Response:
{
  "document_id": "xxx",
  "job_id": "yyy",
  "status": "queued"
}
```

### 4. Job Status Takip
```bash
curl http://localhost:8001/api/ingest/job/yyy/status

# Progress gÃ¶receksin:
{
  "status": "processing",
  "progress": 75,
  "message": "Generating embeddings..."
}
```

---

## ğŸ’° MALÄ°YET TAHMÄ°NÄ°

### Serverless (Pay-per-use)
```
GPU: A4000 Spot $0.25/hr
Ä°ÅŸlem sÃ¼resi: ~15 saniye/dosya
Maliyet/dosya: $0.001

Senaryolar:
- 100 dosya/gÃ¼n   â†’ ~$3/ay
- 500 dosya/gÃ¼n   â†’ ~$15/ay
- 1000 dosya/gÃ¼n  â†’ ~$30/ay
```

### Idle Maliyet
```
Min Workers: 0 â†’ HiÃ§ iÅŸlem yoksa $0
(En ekonomik seÃ§enek!)
```

---

## ğŸ› ï¸ TROUBLESHOOTING

### Problem: "Image not found"
**Ã‡Ã¶zÃ¼m:** Docker Hub'da image public olmalÄ±
```bash
# Docker Hub'da repository'yi public yap:
docker.io â†’ tugberkkalay/doxasense-gpu-worker â†’ Settings â†’ Visibility â†’ Public
```

### Problem: "Container failed to start"
**Ã‡Ã¶zÃ¼m:** Logs kontrol et
```
Runpod Console â†’ Endpoint â†’ Logs
Handler.py'de hata var mÄ± bak
```

### Problem: "Model download timeout"
**Ã‡Ã¶zÃ¼m:** Models image'de pre-download edilmeli
```dockerfile
# Dockerfile'da RUN python -c "..." komutlarÄ± var mÄ± kontrol et
```

---

## ğŸ“Š PERFORMANS BEKLENTÄ°LERÄ°

### Cold Start (Ä°lk Ä°ÅŸlem)
```
Worker baÅŸlatma: ~10 saniye
Model loading: ~0 saniye (pre-loaded!)
Ä°ÅŸlem: ~15 saniye
Toplam: ~25 saniye
```

### Warm Start (Sonraki Ä°ÅŸlemler)
```
Worker zaten Ã§alÄ±ÅŸÄ±yor: 0 saniye
Ä°ÅŸlem: ~15 saniye
Toplam: ~15 saniye
```

### Speedup (CPU vs GPU)
```
PDF (10 sayfa):   120s â†’ 12s  (10x)
Audio (10 dk):    180s â†’ 15s  (12x)
Image:             20s â†’  3s  (7x)
Video (5 dk):     300s â†’ 30s  (10x)
```

---

## âœ… DEPLOYMENT CHECKLIST

- [ ] Docker Desktop kurulu (local'de)
- [ ] Docker Hub hesabÄ± var
- [ ] Runpod hesabÄ± var ($5.21 credit var âœ“)
- [ ] Local'de files indirildi
- [ ] `docker build` Ã§alÄ±ÅŸtÄ±rÄ±ldÄ± (~25 dk)
- [ ] `docker push` yapÄ±ldÄ± (~15 dk)
- [ ] Runpod'da endpoint oluÅŸturuldu
- [ ] Endpoint URL alÄ±ndÄ±
- [ ] `.env` dosyasÄ±na eklendi
- [ ] Backend restart edildi
- [ ] `?use_gpu=true` ile test edildi
- [ ] Ä°lk dosya iÅŸlendi (~25s)
- [ ] Sonraki dosyalar hÄ±zlÄ± (~15s)

---

## ğŸ¯ Ã–ZET

**Toplam SÃ¼re:** ~1 saat
- Build: 25 dakika
- Push: 15 dakika
- Deploy: 5 dakika
- Test: 5 dakika

**SonuÃ§:** 
- âœ… GPU'da 10-15 saniye/dosya
- âœ… CPU'da 108 saniye â†’ 10x speedup!
- âœ… Pay-per-use (ekonomik)
- âœ… Auto-scaling

**BaÅŸarÄ±lar! ğŸš€**
